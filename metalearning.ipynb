{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dimitree54/metalearning/blob/master/metalearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o623dzWzPkWz"
   },
   "source": [
    "# Metalearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au2xx68AP7lE"
   },
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "This document contains research journal and code for metalearning project. The main idea is to replace classical gradient optimizers with some neural-network optimizer (teacher network) to achieve some of this advantages:\n",
    "\n",
    "\n",
    "*   Increase optimizers quality (in speed or final quality)\n",
    "*   Remove some classic optimizer boundaries such as requirement of loss differentiability or backpropogation issues (as vanishing or exploading gradients)\n",
    "*   Application optimization network to itself recoursively to research some unexpected results of such deep self optimization\n",
    "\n",
    "\n",
    "For the education purposes all the code will be written on tesorflow 2.0 and will support all available computation units: CPU, GPU and TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wUDQ-gPGCm5"
   },
   "source": [
    "---\n",
    "## Definitions and abbreviations\n",
    "\n",
    "*   Teacher-network or TN - neural network to replace classical gradient optimizers. \n",
    "*   Student-network or SN - neural network which will be trained by teacher-network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SORUNsXTV534"
   },
   "source": [
    "---\n",
    "\n",
    "## Setting up environment\n",
    "Connecting to the Google Drive and move to working directory to get access to custom script files. Installing packages (needed for current hardware) and importing mailn packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "lEi739agZT6h",
    "outputId": "5ca6349d-71e3-432e-a9bd-08649b24c7b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with CPU device\n",
      "\u001b[K     |████████████████████████████████| 87.9MB 273kB/s \n",
      "\u001b[K     |████████████████████████████████| 501kB 42.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.1MB 37.3MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# setting up working directory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n",
    "#%cd /content/drive/My\\ Drive/Colab\\ Notebooks/metalearning\n",
    "\n",
    "# getting device type\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "    device = \"TPU\"\n",
    "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "elif ('COLAB_GPU' in os.environ and os.environ['COLAB_GPU'] == '1'):\n",
    "    device = \"GPU\"  # TODO support local gpu\n",
    "else:\n",
    "    device = \"CPU\"\n",
    "print(\"Working with {} device\".format(device))\n",
    "\n",
    "# installing correct version of tensorflow 2.0\n",
    "if device == \"GPU\":\n",
    "    !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "else:\n",
    "    !pip install -q tensorflow==2.0.0-beta1\n",
    "    \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "180tzkhpCr1i"
   },
   "source": [
    "---\n",
    "## Data representation\n",
    "Here we will describe data pipeline for both teacher and student neural networks. The purpose of the research is to build unviersal optimizer (teacher-network) supporting different learning tasks and different student-network architectures. So different training tasks and student-network architectures will be considered for teacher-network training and testing. On the other hand, for research purposses, complexity of considering tasks and architectures should be limited.\n",
    "\n",
    "As tasks we will use some classical problems available in module tf.keras.datasets (without NLP datasets):\n",
    "*   MNIST\n",
    "*   CIFAR 10\n",
    "*   CIFAR 100\n",
    "*   Fashion MNIST\n",
    "\n",
    "UPDATE: regression removed\n",
    "Despite most of datasets are image-oriented we will use fully conncted layers because of its simplisty. We will generate random SN architectures and train them in parallel by TN and gradient descent for results comparison. Activation function of all hidden layers will be sigmoid, for last layer activation function fill be task-dependent: softmax for classificaion and\n",
    "\n",
    "The input of the training pairs will be feed forward to SN as is. The TN will take as input state of SN and training pair. Which exactly information about SN will be feed to TN is important question which will be considered below.\n",
    "\n",
    "To make TN applicable for every SN network architecture, input of TN should not be dependent of SN dimensions (such as hidden layer sizes). To achieve such universality we consider all SN weights independently. I.e. TN will take as input local weight information (weight value and input value) and some global information (loss...) and will output new weight value.\n",
    "\n",
    "We will consider as TN inputs folowing candidates in different combinations:\n",
    "*   Weight input and maybe its encoded history\n",
    "*   Weight value and maybe its encoded history\n",
    "*   Encoded information about other neurons in this layer (inputs and values)\n",
    "*   Encoded information about neuron type (for example its activation function)\n",
    "*   Loss value and maybe its encoded history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8y1aO14_uG2d"
   },
   "outputs": [],
   "source": [
    "# preparing data\n",
    "# for SN\n",
    "def get_boston_housing_data():\n",
    "    pass\n",
    "\n",
    "def get_mnist_data():\n",
    "    pass\n",
    "\n",
    "def get_cifar10_data():\n",
    "    pass\n",
    "\n",
    "def get_cifar100_data():\n",
    "    pass\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "    pass\n",
    "\n",
    "# for TN\n",
    "class DataPool:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self):\n",
    "        pass\n",
    "\n",
    "    def get(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmMJalJzgare"
   },
   "source": [
    "To simplify the TN optimization, SN will folow a lot of restrictions:\n",
    "* only fully connected layers\n",
    "* only sigmoid activation function\n",
    "* bias will be emulated constant on layer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Scgiw9X2Ob__"
   },
   "outputs": [],
   "source": [
    "# creating networks\n",
    "MAX_LAYER_SIZE = 128\n",
    "MAX_NUM_LAYERS = 10\n",
    "\n",
    "# net description is a list of num_units in hidden layers\n",
    "def get_tn_description():\n",
    "    return [128, 128, 128]\n",
    "\n",
    "def get_random_net_description():\n",
    "    net_description = []\n",
    "    num_layers = random.randint(1, MAX_NUM_LAYERS)\n",
    "    for _ in num_layers:\n",
    "        net_description.append(random.randint(1, MAX_LAYER_SIZE))\n",
    "    return net_description\n",
    "\n",
    "def fc_layer(inputs, layer_size, weights=None):\n",
    "    \"\"\"\n",
    "    inputs shape is [bs, n]\n",
    "    weights shape is [n + 1, m]\n",
    "    \"\"\"\n",
    "    bs, n = inputs.shape\n",
    "    if weights is None:\n",
    "        weights = tf.Variable(\n",
    "            initial_value=tf.initializers.GlorotNormal()(shape=[n + 1, layer_size]),\n",
    "            trainable-True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    else:\n",
    "        w_n, w_size = weights.shape\n",
    "        assert wn == n and w_size == layer_size\n",
    "    # appending 1 to input to emulate bias:\n",
    "    biased_inputs = tf.pad(inputs,[[0,0],[0,1]],constant_values=1)\n",
    "    outputs = tf.matmul(inputs, weights)\n",
    "    return tf.sigmoid(outputs)\n",
    "\n",
    "def get_sn(inputs, net_description, weights_set=None):\n",
    "    net = inputs\n",
    "    for weights in weights_set:\n",
    "        net = fc_layer(net, weights)\n",
    "    return net\n",
    "\n",
    "def tn(local_state, global_state):\n",
    "    net = tn_body(local_state, global_state)\n",
    "    previous_layer_size = inputs_size\n",
    "    for layer_size in net_description:\n",
    "        tn_head = \n",
    "\n",
    "\n",
    "\n",
    "def create_sn(output_size: int, layer_sizes: list):\n",
    "    \"\"\"\n",
    "    Create random student network.\n",
    "    :param output_size: desired number of output units\n",
    "    :param regression: boolean flag to make model for regression instead of\n",
    "     classification\n",
    "    :max_layer_size: max neurons in hidden layer\n",
    "    :max_layers: max density of network\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    for i in range(randint(0, max_layers)):\n",
    "        model.add(layers.Dense(randint(1, max_layer_size), activation='sigmoid'))\n",
    "    model.add(layers.Dense(output_size, \n",
    "                           activation='linear' if regression else 'sigmoid'))\n",
    "    return\n",
    "\n",
    "def encode_local_state():\n",
    "    pass\n",
    "\n",
    "def encode_global_state():\n",
    "    pass\n",
    "\n",
    "def create_tn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ogwjSgjWl1U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SORUNsXTV534"
   ],
   "include_colab_link": true,
   "name": "metalearning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
